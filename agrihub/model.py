# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15PxqJeKbpP2JcqRMiVSECWiHgeiI4W4s
"""
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

dataset=pd.read_csv('./Crop_recommendation.csv')

dataset

dataset.isnull().sum().sort_values(ascending=False)

dataset.drop(['Unnamed: 8','Unnamed: 9'],axis=1,inplace=True)

dataset

y=dataset['label']

x=dataset.drop('label',axis=1)

from sklearn.feature_selection import VarianceThreshold
var_thres=VarianceThreshold(0)#here we give the thresold value for variance is 0 because above 0 variance the columns will be there but <=0 variance column should be drop here
var_thres.fit(x)

var_thres.get_support() #here no column will be false so that means there is no such column which is constant

"""## determining highly correlated columns"""

import matplotlib.pyplot as plt
plt.figure(figsize=(10,12))
import seaborn as sns
sns.heatmap(x.corr(),annot=True)
plt.show()

def correlation(dataset, thresold):
    column_corr=set()#for unique highly correlated columns whose correlation is greatr than the thresold value giveen
    correlation_matrix=dataset.corr()
    for i in range(len(correlation_matrix.columns)):
        for j in range(i):
            if abs(correlation_matrix.iloc[i,j])>thresold:
                col_name=correlation_matrix.columns[i]
                column_corr.add(col_name)
    return column_corr

highly_correlated_features=correlation(x,0.90)

print(highly_correlated_features) # we'll find no such columns that is highly correlated as 90%

y.unique()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)



from sklearn.feature_selection import mutual_info_classif
# determine the mutual information
mutual_info = mutual_info_classif(x_train, y_train)
mutual_info

mutual_info = pd.Series(mutual_info)
mutual_info.index = x_train.columns
mutual_info.sort_values(ascending=False)


from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(x_train, y_train)

pickle.dump(classifier, open("model.pkl","wb"))

